# TUAB Abnormality Detection Linear Probe Configuration

model:
  checkpoint_path: "models/eegpt/pretrained/eegpt_mcae_58chs_4s_large4E.ckpt"  # relative to BGB_DATA_ROOT
  n_input_channels: 23  # TUAB uses 23 channels
  n_classes: 2  # normal/abnormal
  embed_dim: 512
  n_summary_tokens: 4
  max_norm: 0.25
  freeze_backbone: true

data:
  root_dir: "datasets/external/tuh_eeg_abnormal/v3.0.1/edf"  # relative to BGB_DATA_ROOT
  sampling_rate: 256  # Hz
  window_duration: 30.0  # seconds
  window_stride: 30.0  # no overlap
  normalize: true

training:
  batch_size: 32  # Reduced for MPS/CPU compatibility
  num_workers: 0  # Set to 0 for MPS/debugging
  epochs: 10
  learning_rate: 0.0005  # 5e-4
  weight_decay: 0.01
  # OneCycleLR settings
  pct_start: 0.2
  div_factor: 25  # initial_lr = max_lr / div_factor
  final_div_factor: 10000  # min_lr = initial_lr / final_div_factor

  # Early stopping
  patience: 5
  monitor: "val_auroc"
  mode: "max"

  # Checkpointing
  save_top_k: 3

experiment:
  name: "tuab_linear_probe"
  seed: 42
  precision: 32  # Full precision for MPS compatibility
  accelerator: "auto"  # Will use GPU if available

# Output paths
output:
  checkpoint_dir: "experiments/eegpt_linear_probe/checkpoints"
  log_dir: "experiments/eegpt_linear_probe/logs"
  probe_save_path: "experiments/eegpt_linear_probe/checkpoints/tuab_probe_best.pth"
