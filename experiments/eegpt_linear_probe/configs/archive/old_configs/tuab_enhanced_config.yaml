# Enhanced TUAB configuration matching EEGPT paper specifications
# WARNING: This uses 10.24s windows @ 200Hz - NOT compatible with existing cached data (8s @ 256Hz)
# If you have cached data from previous runs, it will be regenerated!
experiment:
  name: "tuab_enhanced_10s_200hz"
  seed: 42
  precision: 32  # Full precision for stability
  
model:
  backbone:
    name: "eegpt"
    checkpoint_path: "${BGB_DATA_ROOT}/models/eegpt/pretrained/eegpt_mcae_58chs_4s_large4E.ckpt"
    freeze: true
    n_channels: 19  # TUAB standard channels
    
  probe:
    type: "two_layer"
    input_dim: 768  # EEGPT feature dimension
    hidden_dim: 16
    n_classes: 2
    dropout: 0.5
    use_channel_adapter: true
    channel_adapter_in: 19
    channel_adapter_out: 19
    max_norm: 1.0  # Weight constraint
    
data:
  dataset: "tuab"
  root_dir: "${BGB_DATA_ROOT}/datasets/external/tuh_eeg_abnormal/v3.0.1/edf"
  cache_dir: "${BGB_DATA_ROOT}/cache/tuab_enhanced"
  
  # STANDARDIZED: Using 8s @ 256Hz for consistency
  window_duration: 8.0    # 8 seconds = 2048 samples @ 256Hz
  window_stride: 4.0      # 50% overlap for training
  sampling_rate: 256      # 256 Hz standard
  
  # Standard preprocessing
  bandpass_low: 0.5       # Standard low
  bandpass_high: 50.0     # Standard high
  notch_filter: null      # No notch for standard config
  
  # Data loading
  batch_size: 64         # Reduced from 100 to prevent OOM (will use gradient accumulation)
  num_workers: 2         # Further reduced to prevent system OOM
  pin_memory: true
  persistent_workers: false  # Disable to save memory
  
  # Channel configuration
  channel_names: [
    'FP1', 'FP2', 'F7', 'F3', 'FZ', 'F4', 'F8',
    'T3', 'C3', 'CZ', 'C4', 'T4',  # Using old naming for TUAB
    'T5', 'P3', 'PZ', 'P4', 'T6',
    'O1', 'O2'  # 19 channels total
  ]
  
  # AutoReject settings (disabled by default)
  use_autoreject: false
  ar_cache_dir: "${BGB_DATA_ROOT}/cache/autoreject"
  ar_fit_samples: 200
  ar_n_interpolate: [1, 4]
  ar_consensus: 0.1
  
  # Cache mode - set to "readonly" after building cache
  cache_mode: "readonly"  # Train cache is complete!
  
training:
  epochs: 20             # Reasonable for baseline test
  learning_rate: 5e-4
  weight_decay: 0.05     # Match paper (was 0.01)
  
  # Warmup settings
  warmup_epochs: 5       # Match paper
  warmup_lr: 1e-6
  min_lr: 1e-6
  
  # Optimizer
  optimizer: "adamw"
  adam_eps: 1e-8
  adam_betas: [0.9, 0.999]
  
  # Gradient clipping
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2  # 64 * 2 = 128 effective batch size (close to paper's 100)
  
  # Layer decay for different learning rates
  layer_decay: 0.65      # Match paper
  
  # Scheduler  
  scheduler: "onecycle"
  pct_start: 0.1         # 10% warmup
  
  # Early stopping
  patience: 10
  monitor: "val_auroc"
  mode: "max"
  
  # Validation
  val_check_interval: 0.5  # Check twice per epoch
  
  # Model averaging
  use_ema: false  # Paper doesn't mention EMA
  
evaluation:
  metrics: ["auroc", "accuracy", "balanced_accuracy", "f1_weighted"]
  save_predictions: true
  
logging:
  log_every_n_steps: 50
  save_top_k: 3          # Save best 3 checkpoints
  
# Hardware
accelerator: "gpu"
devices: 1
distributed: false