# Enhanced TUAB configuration matching EEGPT paper specifications
experiment:
  name: "tuab_memsafe_wsl"
  seed: 42
  precision: 16  # Mixed precision for memory savings
  
model:
  backbone:
    name: "eegpt"
    checkpoint_path: "${BGB_DATA_ROOT}/models/eegpt/pretrained/eegpt_mcae_58chs_4s_large4E.ckpt"
    freeze: true
    n_channels: 19  # TUAB standard channels
    
  probe:
    type: "two_layer"
    input_dim: 768  # EEGPT feature dimension
    hidden_dim: 16
    n_classes: 2
    dropout: 0.5
    use_channel_adapter: true
    channel_adapter_in: 19
    channel_adapter_out: 19
    max_norm: 1.0  # Weight constraint
    
data:
  dataset: "tuab"
  root_dir: "${BGB_DATA_ROOT}/datasets/external/tuh_eeg_abnormal/v3.0.1/edf"
  cache_dir: "${BGB_DATA_ROOT}/cache/tuab_enhanced"
  
  # Window specifications - reduced for memory
  window_duration: 5.12   # 5.12 seconds = 1024 samples @ 200Hz
  window_stride: 2.56     # 50% overlap for training
  sampling_rate: 200     # 200 Hz to match paper (was 256)
  
  # Preprocessing to match paper
  bandpass_low: 0.1      # Paper uses 0.1-75 Hz (was 0.5)
  bandpass_high: 75.0    # Paper uses 75 Hz (was 50)
  notch_filter: 50.0     # Add notch filter
  
  # Data loading
  batch_size: 16         # Very conservative for WSL
  num_workers: 0         # CRITICAL: Avoid WSL deadlocks
  pin_memory: false      # Save host memory
  persistent_workers: false  # Disable to save memory
  prefetch_factor: null  # Disable prefetching to avoid WSL hangs
  
  # Channel configuration
  channel_names: [
    'FP1', 'FP2', 'F7', 'F3', 'FZ', 'F4', 'F8',
    'T3', 'C3', 'CZ', 'C4', 'T4',  # Using old naming for TUAB
    'T5', 'P3', 'PZ', 'P4', 'T6',
    'O1', 'O2'  # 19 channels total
  ]
  
training:
  epochs: 50             # Match paper (was 20)
  learning_rate: 5e-4
  weight_decay: 0.05     # Match paper (was 0.01)
  
  # Warmup settings
  warmup_epochs: 5       # Match paper
  warmup_lr: 1e-6
  min_lr: 1e-6
  
  # Optimizer
  optimizer: "adamw"
  adam_eps: 1e-8
  adam_betas: [0.9, 0.999]
  
  # Gradient clipping
  gradient_clip_val: 1.0
  accumulate_grad_batches: 8  # 16 * 8 = 128 effective batch size
  
  # Layer decay for different learning rates
  layer_decay: 0.65      # Match paper
  
  # Scheduler  
  scheduler: "onecycle"
  pct_start: 0.1         # 10% warmup
  
  # Early stopping
  patience: 10
  monitor: "val_auroc"
  mode: "max"
  
  # Validation
  val_check_interval: 0.5  # Check twice per epoch
  
  # Model averaging
  use_ema: false  # Paper doesn't mention EMA
  
evaluation:
  metrics: ["auroc", "accuracy", "balanced_accuracy", "f1_weighted"]
  save_predictions: true
  
logging:
  log_every_n_steps: 50
  save_top_k: 3          # Save best 3 checkpoints
  
# Hardware
accelerator: "gpu"
devices: 1
distributed: false