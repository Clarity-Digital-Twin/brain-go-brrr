# EEGPT Linear Probe Training - BULLETPROOF VERSION

## ğŸ”´ STATUS: 10 CRITICAL BUGS FIXED (2025-08-09)

**Complete audit found and fixed ALL issues:**
1. âœ… `cycle_momentum=False` for AdamW (CRITICAL!)
2. âœ… Per-batch scheduler stepping
3. âœ… No start_epoch reset bug
4. âœ… Accumulation-aware total_steps
5. âœ… Global step tracking
6. âœ… Optimizer LR logging
7. âœ… RNG state saving
8. âœ… Extensive sanity checks
9. âœ… Non-blocking transfers
10. âœ… Gradient norm monitoring

**Use ONLY:** `train_paper_aligned_BULLETPROOF.py`

See `COMPLETE_AUDIT_FINDINGS.md` for full details.

## ğŸ¯ Mission: Achieve Paper-Level Performance

**Target**: AUROC â‰¥ 0.869 (paper performance with 4-second windows)

## ğŸš€ Quick Start (BULLETPROOF VERSION)

```bash
# 1. Test scheduler (dry run)
python train_paper_aligned_BULLETPROOF.py --dry_run

# 2. Launch bulletproof training
./LAUNCH_BULLETPROOF.sh

# 3. Monitor progress
tmux attach -t eegpt_bulletproof

# 4. Verify LR is changing
tail -f logs/BULLETPROOF_*.log | grep 'LR:'
```

**Expected LR progression:**
- Step 1: ~0.00012 (initial)
- Step 100: Increasing (warmup)
- Step 500: ~0.003 (peak)
- Step 2900+: Decreasing (annealing)
- Final: ~0.000003

## ğŸ“ Clean Directory Structure

```
experiments/eegpt_linear_probe/
â”œâ”€â”€ configs/                      # Training configurations
â”‚   â”œâ”€â”€ tuab_4s_paper_aligned.yaml  # âœ… ACTIVE - Paper-aligned 4s config
â”‚   â”œâ”€â”€ tuab_8s_temp.yaml           # 8s config (suboptimal)
â”‚   â””â”€â”€ archive/                    # Old configs
â”œâ”€â”€ output/                       # Training outputs
â”‚   â””â”€â”€ tuab_4s_paper_aligned_*/    # Current training
â”œâ”€â”€ archive/                      # Obsolete/failed attempts
â”‚   â””â”€â”€ old_scripts/              # Deprecated scripts
â”œâ”€â”€ train_paper_aligned.py       # âœ… MAIN training script
â”œâ”€â”€ smoke_test_paper_aligned.py  # Pre-flight checks
â”œâ”€â”€ custom_collate_fixed.py      # Handles variable channels
â”œâ”€â”€ launch_paper_aligned_training.sh  # Launch script
â””â”€â”€ *.md                          # Documentation
```

## ğŸ”‘ Critical Insights

### Why 4-Second Windows Are Essential

| Window Size | AUROC | Status | Notes |
|------------|-------|--------|-------|
| **4 seconds** | **0.869** | **âœ… Paper** | EEGPT pretrained on 4s |
| 8 seconds | ~0.81 | âŒ Too low | Mismatched with pretraining |

**The pretrained EEGPT model expects 4-second windows!**

### Architecture

```
Input (4s @ 256Hz) â†’ EEGPT (frozen) â†’ Linear Probe â†’ Binary Classification
     1024 samples       512-dim            2 classes
                       features         (normal/abnormal)
```

## âš ï¸ Common Pitfalls & Solutions

| Problem | Solution |
|---------|----------|
| PyTorch Lightning hangs | Use pure PyTorch (`train_paper_aligned.py`) |
| Missing cache index | Copy from 8s cache or build new |
| Channel count mismatch | Use `custom_collate_fixed.py` |
| Wrong window size | MUST use 4 seconds |
| Old channel names | TUAB uses T3/T4/T5/T6 (handled automatically) |

## ğŸ“Š Performance Benchmarks

| Metric | Current | Target | Paper |
|--------|---------|--------|-------|
| AUROC | TBD (training) | â‰¥0.85 | 0.869 |
| Balanced Acc | TBD | >80% | 85.4% |
| Window Size | 4s | 4s | 4s |
| Epochs | 0/200 | - | 200 |

## ğŸ› ï¸ Key Configuration

```yaml
# configs/tuab_4s_paper_aligned.yaml
data:
  window_duration: 4.0  # CRITICAL: Must be 4 seconds
  window_stride: 2.0    # 50% overlap for training
  sampling_rate: 256
  
model:
  backbone:
    checkpoint_path: eegpt_mcae_58chs_4s_large4E.ckpt
  probe:
    input_dim: 512  # EEGPT embedding dimension
```

## ğŸ“‹ Monitoring Commands

```bash
# Live training view
tmux attach -t eegpt_4s_final

# Check process
ps aux | grep train_paper_aligned

# Watch logs
tail -f output/tuab_4s_paper_aligned_*/training.log | grep -E "Epoch|AUROC"

# GPU usage
watch -n 1 nvidia-smi
```

## ğŸ“š Documentation

- `TRAINING_STATUS.md` - Live training updates
- `ISSUES_AND_FIXES.md` - Problems encountered & solutions
- `SETUP_COOKBOOK.md` - Detailed setup guide

## ğŸ¯ Success Criteria

- [ ] AUROC â‰¥ 0.869 on validation set
- [ ] Stable training (no NaN/divergence)
- [ ] Reproducible results (seed=42)
- [ ] Saved best checkpoint

## ğŸš€ Next Steps

1. **Let current training complete** (3-4 hours)
2. **Evaluate on test set** once training finishes
3. **Save best model** for production inference
4. **Document final results** in TRAINING_STATUS.md

---

**Remember**: The key to success is using 4-second windows to match the EEGPT pretraining!